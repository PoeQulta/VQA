# Visual Question Answering on the VizWiz Dataset

This repository contains the implementation of the paper Less Is More: Linear Layers on CLIP Features as Powerful VizWiz Model. The model leverages OpenAI's CLIP model to encode both text and images.
[Kaggle Notebook](https://www.kaggle.com/code/polaqulta/ptrnrec-vqa)
## Table of Contents
- Introduction
- Results

## Introduction
The project aims to implement a Visual Question Answering (VQA) model on the VizWiz dataset. The model is based on the paper ["Less Is More: Linear Layers on CLIP Features as Powerful VizWiz Model"](https://arxiv.org/abs/2206.05281). It uses OpenAI's CLIP model to encode both text and images, which are then used to answer questions about the images.

## Results
After 10 epochs of training the Answer Accuracy is 0.32 while the Type Accuracy is 0.77 this roughly matches the results obtained in the paper.
